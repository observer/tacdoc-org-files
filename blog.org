* Configuration                                                    :noexport:
** blog
   #+title:
   #+author:
** ox-hugo settings
   #+hugo_base_dir: ~/tactical-documentation/
   #+hugo_section: post
   #+hugo_weight: auto
   #+hugo_auto_set_lastmod: t
** Macros
*** Sidenotes
    Careful! You have to escape commas: =,= becomes =\,=

    This defines how sidenotes work:
    #+macro: sidenote <label class="margin-toggle sidenote-number"></label><span class="sidenote"> $1 </span>
    In order to use it use:
    #+begin_example
      {{{sidenote(This is a sidenote.)}}}
    #+end_example
*** Marginnotes
    The same as sidenotes
    #+macro: marginnote </label><span class="marginnote"> $1 </span>

*** Mathjax 
    In order to enable Mathjax for a blog entry, we need to make sure
    the mathjax partial is loaded. Enabling it on a per blog entry
    base is better than loading it everytime because it is part of the
    default footer. In order to do so we use a shortcode defined in
    our theme.

    #+macro: enable_mathjax {{< enableMathjax >}}

    Of course you can also use the shortcode directly, but typing
    ={{{enable_mathjax}}}= is faster than typing ={{< enableMathjax>}}=.
** config.toml
   The =config.toml= file I use currently contains the following:
   #+begin_example 
     baseURL = "https://tactical-documentation.github.io"
     languageCode = "en-us"
     title = "tactical-documentation"theme = "hugo-tacdoc-theme"

     pygmentsstyle       = "manni"
     pygmentscodefences  = true
     pygmentsUseClassic = false              # Force use of chroma

     # by adding this categories are disabled, tags are enough for now
     [taxonomies]
       tag = "tags"
   #+end_example
   
* How to use this document
  In order to use this document, we need to make sure that a few
  things are set for each blog post:
** =PROPERTY= drawer
   Just an example, the most important part being to set the
   =EXPORT_HUGO_CUSTOM_FRONT_MATTER= variable correctly, not sure if
   this can be done any other way.
   #+BEGIN_EXAMPLE
   ,  :PROPERTIES:
   ,  :EXPORT_FILE_NAME: filename-of-the-post
   ,  :EXPORT_HUGO_CODE_FENCE: t
   ,  :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :date "\"1999-01-01\""
   ,  :END:
   #+END_EXAMPLE
** sidenotes
   Careful! You have to escape commas: =,= becomes =\,=

   In order to use the sidenotes in the blog, use the sidenote macro:
   #+BEGIN_EXAMPLE
     {{{sidenote(This is displayed at the side of the page.)}}}
   #+END_EXAMPLE
* Static Pages
  This section contains the static pages of the blog, that appear in
  the header, such as the about page, maybe more at some point in time
** about
   :PROPERTIES:
   :EXPORT_FILE_NAME: about
   :EXPORT_HUGO_SECTION: .
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :date "\"\""
   :END:

   This page contains some general information about this blog, first
   a short overview of this project, then a little FAQ-ish section
   with stuff you might want to know about this site and finally there
   is a small showcase of this hugo theme, that I've put together with
   pretty much no background knowledge.

   This project consists of three github repositories{{{sidenote(At
   least until I get around to merging the theme and the org
   repository so that the org file can be tangled in order to generate
   the theme files.)}}}, [[https://github.com/tactical-documentation/tactical-documentation.github.io][one of them]] housing the files for this
   statically linked site, [[https://github.com/tactical-documentation/hugo-tacdoc-theme][another one]] containing the theme for this
   site and the [[https://github.com/tactical-documentation/org][third one]] containing the org source of this site. I
   use [[https://www.gnu.org/software/emacs/][emacs]] to write this content using the amazing [[https://orgmode.org/][org]] file format,
   [[https://github.com/kaushalmodi/ox-hugo][ox-hugo]] to export the content into whatever file structure [[https://gohugo.io][hugo]]
   uses to generate this statically linked site.
**** Disclaimer
     As far as I understand there might be a few things that I am
     obliged to mention, so here they are:
     - I do not assume any kind of responsibility for anything you do,
       think, understand or feel before, after or during the time you
       read any of this content. I just want a place to write stuff
       down and I sincerly hope all people, who get to read it just
       have a nice time reading it or benefit from what I write down.
     - Most of the content I will put on this site is probably going
       to be fairly technical, you should not under any circumstaces
       try any of it out if you don't know exactly what you are
       doing. Even then you should always check if the information you
       read is correct or applicable to your personal case. I really
       hope that whatever I write down might help you solve a problem
       or that you just have a great time reading it, but just in case
       you should always try to check if what you read is actually
       correct.
     - As the times, when the internet didn't forget anything seem to
       have passed, there is another topic I might have to mention: I
       link to other sources and I am not responsible for whatever
       happens when you click on those links or where-ever these links
       might or might not lead, because a site has been remodeled,
       moved, died or been replaced by another one or anything
       else. You shall not hold me accountable to anything, when you
       click on a link on this site. I will try to kepp dead links up
       to date, but it really is not a priority or something that I do
       actively. Also since this is a very simple static page,there is
       no intermediate warning page that makes you aware of the fact
       that you are about to leave this site. If leaving this site
       using a link is something that concerns you, don't follow any
       links on this site.
**** Privacy
     I do not collect any of your private data. I'm solely a user of
     the github pages service, where this site is hosted. In case you
     have any concerns you might have as to how your private data is
     collected, stored or used, you should get in touch with github
     (whose very detailed privacy statement is at the time I'm writing
     this available [[https://help.github.com/en/articles/github-privacy-statement][here]]).
**** Where is this blog hosted?
     It is hosted on github pages, they have a privacy statement,
     which at the time I'm writing this resides [[https://help.github.com/en/articles/github-privacy-statement][here]] and where you can
     in detail read about everything they do with data. This site will
     not use any external services besides github pages, there is no
     commenting system and no analytics script running. This is just a
     simple static site with information on it.
**** What services are used when this site is accessed?
     I guess this part might be fairly interesting to some people, so
     here it is:

     The site does not use any external services, there are no ads or
     cookies. You can look at the source of the hugo theme [[https://github.com/tactical-documentation/hugo-tacdoc-theme][here]] and at
     the source of this site [[https://github.com/tactical-documentation/tactical-documentation.github.io][here]]. A bit of CSS is used for styling,
     you can take a look at it at the theme repository.

     For the most part, no Javascript is used on this site, with the
     sole exception currently being [[https://www.mathjax.org/][Mathjax]], which is used to display
     rendered version of [[https://www.latex-project.org/][LaTeX]] code on some of the posts
     {{{sidenote(this one included\, due to the little theme showcase
     below)}}}. The Mathjax files used in this blog are not hosted by
     an external CDN, instead they are also hosted with the rest of
     the static files and reside in the repository of this site.

     Additional Javascript may be added at a later point of time,
     however I'll only add Javascript in order to enable missing
     functionality, such as Mathjax for the purpose of rendering LaTeX
     formulas. If I do so I'll add the necessary files to the source
     repo as well as to avoid generating any external
     connections. I'll also try to remember to append such information
     to this page.

     If you want to access the content of this site, without a browser
     or in text form, you can always just grab a copy of the
     repository containing the org source of the content and read it
     in your favourite pager or text editor or just try to rebuild the
     site locally and view it served from your own machine (see the
     next section for pointers on how to do so).
**** How I build this site?
     First we have to create a new Hugo project, we can follow the
     getting started part of their documentation for this.

     Once the project has been created, we can add our theme in the
     theme directory, make sure the =config.toml= {{{sidenote(I am
     actually keeping my config.toml in my org repo\, together with
     the rest of the content\, so keeping any additional repository
     containing files that hugo has generated would be pretty
     pointless as really only the public/ part of this repository is
     worth keeping.)}}} is actually using it and run =hugo= from within the
     project folder to create the =public/= directory.

     When it comes to the theme, we might want to update the version
     of mathjax it is using and we might want to make sure the
     mathjax-fonts are available inside the =static/fonts/= directory of
     the theme, although the missing fonts error I encountered when
     building this locally is something that went away after a rebuild
     and without having to supply the fonts, so this part might not be
     necessary.

     When that's done, we can focus on the content: open up the org
     file in emacs, add some content and =C-c C-e H A=, then head over
     to the hugo project, run =hugo= and check if it looks as expected.
*** Theme Preview
 # enable Mathjax for this post:
 {{{enable_mathjax}}}

 This part of the page is a bit of a showcase for the theme used in
 this blog, I've pretty much cargo-culted it from various sources,
 including the following hugo themes:
 - [[https://github.com/davidhampgonsalves/hugo-black-and-light-theme][black-and-light]]
 - [[https://github.com/alanorth/hugo-theme-tufte-css][hugo-theme-tufte-css]]
 - [[https://github.com/bobfp/hugo-slick][slick]]

 A few interesting things from the org file of the ox-hugo page:

 Testing a sidenote{{{sidenote(This text should appear at the side of
 the text)}}} inside a block of text, which contains another
 sidenote{{{sidenote(which is probably also displayed at the side of
 the page)}}}.

 There are also margin notes{{{marginnote(Note how this shouldn't be numbered.)}}}.

 So here is an example block:
 #+BEGIN_EXAMPLE
   #+setupfile: doc-setupfile.org

   #+macro: imageclick [[file:./static/images/$1][file:/images/$1]]
   #+macro: doc [[/doc/$1/$2][$3]]
 #+END_EXAMPLE

 Also how do quotes look like?
 #+BEGIN_QUOTE
   These things happened. They were glorious and they changed the
   world... and then we fucked up the end game.
   - C.W.
 #+END_QUOTE

 How about Footnotes [fn::This is a footnote]?

 Images:
 #+caption: This is an image.
 [[./img/htop.png]]

 Lists:
 - item
 - another item
   - item
   - bla
 - bla

 Tables:
 #+caption: This is a caption
 | This | is | a | Table |
 |------+----+---+-------|
 |    1 |  2 | 3 |     4 |
 |    5 |  6 | 7 |     8 |

 Task Lists
 - [ ] task
 - [X] task
 - [ ] task
 - [X] task
 - [ ] task

 Formulas using Mathjax $\log_{ab}^{x} = x * \frac{2}{5}$

 Next lets look at source code blocks:

 Python:
 #+BEGIN_SRC python
   for i in bla:
     print i;
 #+END_SRC

 Bourne shell:
 #+BEGIN_SRC sh
   for i in $(ls ~/some/path | grep foo)
   do 
     echo -e "\n Here is some output : $i"
   done
 #+END_SRC

 Bash: 
 #+BEGIN_SRC bash
   for i in $(ls ~/some/path | grep foo)
   do 
     echo -e "\n Here is some output : $i"
   done
 #+END_SRC

 HTML:
 #+BEGIN_SRC html
   <head> Bla </head>
   <body> Bleh </body>
 #+END_SRC

 C++:
 #+BEGIN_SRC c++
   #import <std.h>

   void main () {
     sdt::cout << "bla" << var << std::endl;
   }
 #+END_SRC

 emacs-lisp:
 #+BEGIN_SRC emacs-lisp
   (defun a-function (a-list)
       "This makes no sense"
     (interactive)
     (dolist (item a-list)
       (print item)))
 #+END_SRC
** src
   :PROPERTIES:
   :EXPORT_FILE_NAME: src
   :EXPORT_HUGO_SECTION: .
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :date "\"\""
   :END:
   The source of this site is available on github:
   1. [[https://github.com/tactical-documentation/tactical-documentation.github.io][tactical-documentation.github.io]]::this is the repository housing
      all of the site you're looking at right now.
   2. [[https://github.com/tactical-documentation/hugo-tacdoc-theme][hugo-tacdoc-theme]]::this contains the theme this site is using.
   3. [[https://github.com/tactical-documentation/org][org]]::this is the link to the actual content of this site written
      in org.
     
   The site is generated using [[https://www.gnu.org/software/emacs/][emacs]], [[https://orgmode.org/][orgmode]], [[https://github.com/kaushalmodi/ox-hugo][ox-hugo]] and [[https://gohugo.io/][hugo]].
* Posts
** Encrypting Proxmox VE 6: ZFS, LUKS, systemd-boot and Dropbear
   :PROPERTIES:
   :EXPORT_FILE_NAME: proxmoxve6-zfs-luks-systemdboot-dropbear
   :EXPORT_HUGO_TAGS: proxmox zfs luks systemd-boot dropbear
   :EXPORT_HUGO_CODE_FENCE: t
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :date "\"2019-08-23\""
   :END:
   This describes how to set up a fully encrypted Proxmox VE 6 host
   with ZFS root and unlocking it remotely using the dropbear ssh
   server. Also it describes how you can do that, while keeping
   systemd-boot and thus also the pve tooling intact{{{sidenote(I'm
   not sure if the pve tooling still works if you replace systemd-boot
   with grub\, which seems to be the common solution to setting up
   this kind of setup\,maybe it does)}}}.
*** Overview
    We are going to do the following:
    1. Install Proxmox VE 6 on our machine
    2. Minimally configure the Installation
    3. Encrypt the Installation:
       1. Remove a Disk from the ZFS-Pool
       2. Encrypt the Disk with LUKS
       3. Add it back to the ZFS Pool
       4. Repeat until all disks are encrypted
    4. Set up Dropbear and Systemd-boot to enable remote unlocking
*** Prerequisites
    There really only is one prerequisite apart from having a machine
    you want to install Proxmox onto: You need a second harddrive,
    which we will setup in a ZFS RAID1 configuration. If you don't
    want to have your root devices mirrored, you will still need a
    second drive that you can use as a temporary mirrored root device,
    otherwise you'd have to install and set up an encrypted debian and
    then install proxmox on top of that.

    Apart from that I'll assume that you are probably fairly familiar
    with how full disk encryption works on linux systems, if not you
    might want to read up on that before you start messing around with
    any hardware. Please don't try this out on a production system,
    if you don't exactly know what you're doing.
*** Installing Proxmox VE 6
    The only thing you have to make sure is to set up the ZFS RAID 1
    during the installation. The rest should be pretty much
    straight-forward.
*** Minimal post-installation
    For some odd reason =PATH= in a regular shell is different from =PATH=
    in the javascript terminal from the webinterface. You might want
    to take care of that:
    #+begin_src bash
      echo "export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" >> ~/.bashrc
    #+end_src

    Remove the subscription popup notice ([[https://johnscs.com/remove-proxmox51-subscription-notice/][source]]):
    #+begin_src bash
      sed -i.bak "s/data.status !== 'Active'/false/g" /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js && systemctl restart pveproxy.service
    #+end_src

    Set up the community repositories:
    #+begin_src bash
      rm /etc/apt/sources.list.d/pve-enterprise.list
      echo 'deb http://download.proxmox.com/debian/pve buster pve-no-subscription' > pve-community.list
    #+end_src

    Update the host:
    #+begin_src bash
      apt update
      apt upgrade
    #+end_src
*** Encrypt your installation
    This is partly taken over from [[https://forums.servethehome.com/index.php?threads/proxmox-zfs-encryption-guide-work-in-progress.23004/#post-215138][this wonderful post]]{{{sidenote(The
    GRUB_ENABLE_CRYPTODISK option that is mentioned in the [[https://forums.servethehome.com/index.php?threads/proxmox-zfs-encryption-guide-work-in-progress.23004/#post-215138][forum post]]
    does not apply here\, since the boot partition is not
    encrypted. If you want this level of security\, then this is
    probably not the right guide for you. Also from my understanding
    encrypting the boot partition means that you can't use dropbear to
    unlock the system remotely since nothing has booted so far. It is
    a pretty nice way to set up fully encrypted laptops though\, so
    you should definitely look into this if you haven't already!)}}}.

    Right after the installation the host should look similiar to this
    (=lsblk=):
    #+begin_example bash
      NAME            MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
      sda               8:0    0 465.8G  0 disk
      ├─sda1            8:1    0  1007K  0 part
      ├─sda2            8:2    0   512M  0 part
      └─sda3            8:3    0 465.3G  0 part
      sdb               8:16   0 931.5G  0 disk
      sdc               8:32   0 931.5G  0 disk
      sdd               8:48   0 465.8G  0 disk
      ├─sdd1            8:49   0  1007K  0 part
      ├─sdd2            8:50   0   512M  0 part
      └─sdd3            8:51   0 465.3G  0 part
    #+end_example
    The third partition of both harddrives contains our installation,
    the first and second are the boot and efi partitions.

    =zpool status= should return something like this:
    #+begin_example bash
              NAME             STATE     READ WRITE CKSUM
              rpool            ONLINE       0     0     0
                mirror-0       ONLINE       0     0     0
                  ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3  ONLINE       0     0     0
                  ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3        ONLINE       0     0     0
    #+end_example

    You might want to install =cryptsetup= at this point:
    #+begin_src bash
      apt install cryptsetup
    #+end_src

    Remove the first partition from =rpool=, then encrypt it, mount it
    to =/dev/mapper/cryptrpool1= and reattach it to =rpool=:
    #+begin_src bash
      zpool detach rpool ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3
      cryptsetup luksFormat /dev/disk/by-id/ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3
      cryptsetup luksOpen /dev/disk/by-id/ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 cryptrpool1
      zpool attach rpool ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 cryptrpool1
    #+end_src

    Wait until the =scan= line of =zpool status= displays that the drive
    has been resilvered successfully. You should see something
    similiar to this:
    #+begin_example
      scan: resilvered 1022M in 0 days 00:00:04 with 0 errors on Wed Aug 21 17:27:55 2019
    #+end_example

    Now repeat this step with the other drive:
    #+begin_src bash

      zpool detach rpool ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3
      cryptsetup luksFormat /dev/disk/by-id/ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3
      cryptsetup luksOpen /dev/disk/by-id/ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 cryptrpool2
      zpool attach rpool cryptrpool1 cryptrpool2
    #+end_src

    At this point =lsblk= should output something like this:
    #+begin_example bash                                                                                                                                                                                                                 
      NAME            MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
      sda               8:0    0 465.8G  0 disk  
      ├─sda1            8:1    0  1007K  0 part  
      ├─sda2            8:2    0   512M  0 part  
      └─sda3            8:3    0 465.3G  0 part  
        └─cryptrpool1 253:0    0 465.3G  0 crypt 
      sdb               8:16   0 931.5G  0 disk  
      sdc               8:32   0 931.5G  0 disk  
      sdd               8:48   0 465.8G  0 disk  
      ├─sdd1            8:49   0  1007K  0 part  
      ├─sdd2            8:50   0   512M  0 part  
      └─sdd3            8:51   0 465.3G  0 part  
        └─cryptrpool2 253:1    0 465.3G  0 crypt
    #+end_example                                                                                                                                                                                                                        
                                                                                                                                                                                                                                         
    And =zpool status= should return something like this:                                                                                                                                                                                      
    #+begin_example bash                                                                                                                                                                                                                 
      NAME             STATE     READ WRITE CKSUM
      rpool            ONLINE       0     0     0
        mirror-0       ONLINE       0     0     0
          cryptrpool1  ONLINE       0     0     0
          cryptrpool2  ONLINE       0     0     0
    #+end_example    


    Next we want to set up =/etc/crypttab=, use =blkid= to get the
    =PARTUUID= from both harddrives:
    #+begin_src bash
      blkid -s PARTUUID -o value /dev/disk/by-id/ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3
      blkid -s PARTUUID -o value /dev/disk/by-id/ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3
    #+end_src

    Then add them to =/etc/crypttab= {{{sidenote(=caliban= is the name of my proxmox host.)}}}:
    #+begin_example
      root@caliban:~# cat /etc/crypttab
      # <target name> <source device>                                <key file>      <options>
        cryptrpool1   PARTUUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX  none            luks,discard,initramfs
        cryptrpool2   PARTUUID=YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY  none            luks,discard,initramfs
    #+end_example

    Then update the initramfs and make sure it is put on the boot
    partition (this is where we deviate from the forum post I've linked
    above):
    #+begin_src bash
      update-initramfs -u -k all
      pve-efiboot-tool refresh
    #+end_src
    In case you're wondering at this point, yes I'm also getting the
    =cryptsetup= error message on running =update-initramfs=, it still works
    though:
    #+begin_example
      cryptsetup: ERROR: Couldn't resolve device rpool/ROOT/pve-1
      cryptsetup: WARNING: Couldn't determine root device
    #+end_example

    Now you should be able to reboot and unlock the ZFS partitions by
    entering the passphrase.
*** Setting up Dropbear to remotely unlock the partition
    Now to the fun part! Since we aren't using =grub= here, we have to take
    a few different steps from what we usually do in this kind of
    setup.

    Here are a few interesting links you might want to look into as well:
    - [[https://www.pbworks.net/ubuntu-guide-dropbear-ssh-server-to-unlock-luks-encrypted-pc/][This]] nicely explains how to use the keys Dropbear already generates on
      install instead of recreating them.
    - The freedesktop page on [[https://www.freedesktop.org/wiki/Software/systemd/systemd-boot/][systemd-boot]]
    - [[https://adfinis-sygroup.ch/en/blog/decrypt-luks-devices-remotely-via-dropbear-ssh/][This little article]] on setting up =archlinux= with =dropbear= does
      not fully apply to our Proxmox case, but it gives enough
      information on how we can tell =systemd-boot= to tell the kernel
      to start with the options we want{{{sidenote(unlike the article
      states\, we need to use the udev name for assigning the IP and I
      was getting error messages\, when supplying nameserver IPs)}}}.

    First install =dropbear= and =busybox=:
    #+begin_src bash
      apt install dropbear busybox
    #+end_src

    In =/etc/initramfs-tools/initramfs.conf= enable busybox:
    #+begin_example
      root@caliban:~# cat /etc/initramfs-tools/initramfs.conf | grep ^BUSYBOX
      BUSYBOX=y
    #+end_example

    Then convert the dropbear keys:
    #+begin_src bash
      cd /etc/dropbear-initramfs/
      /usr/lib/dropbear/dropbearconvert dropbear openssh dropbear_rsa_host_key id_rsa
      dropbearkey -y -f dropbear_rsa_host_key | grep "^ssh-rsa " > id_rsa.pub
    #+end_src

    And add your public key to the authorized keys:
    #+begin_src bash
      vi /etc/dropbear-initramfs/authorized_keys
    #+end_src

    Make sure =dropbear= starts by toggling the =NO_START= value in
    =/etc/default/dropbear=.
    #+begin_example 
      root@caliban:~# cat /etc/default/dropbear | grep ^NO_START
      NO_START=0
    #+end_example

    Finally configure =dropbear= to use a different Port than 22 in order to
    avoid getting the MITM warning, by changing the =DROPBEAR_OPTIONS= value
    in /etc/dropbear-initramfs/config:
    #+begin_example
      root@caliban:~# cat /etc/dropbear-initramfs/config | grep ^DROPBEAR_OPTIONS
      DROPBEAR_OPTIONS="-p 12345"
    #+end_example

    You can then set up two entries in your =~/.ssh/config=:
    #+begin_example
      $ cat ~/.ssh/config
      Host *
      ServerAliveInterval 120

      Host unlock_caliban
        Hostname 1.2.3.4
        User root
        Port 2222

      Host caliban
        Hostname 1.2.3.4
        Port 22
    #+end_example
    At this point I noticed, that only the third partition of both of the
    harddrives with the rpool were mounted. When mounting a boot
    partition, I found that there were systemd-boot configuration files,
    but they seemed to be autogenerated by Proxmox, whenever
    =pve-efiboot-tool refresh= was run. So I looked into
    =/usr/sbin/pve-efiboot-tool=, and followed the code until I came out in
    =/etc/kernel/postinst.d/zz-pve-efiboot=, which contains the code that
    generates the systemd-boot configuration files:
    #+begin_src bash
      # [...]
      for kver in ${BOOT_KVERS}; do

          linux_image="/boot/vmlinuz-${kver}"
          initrd="/boot/initrd.img-${kver}"

          if [ ! -f "${linux_image}" ]; then
              warn "No linux-image ${linux_image} found - skipping"
              continue
          fi
          if [ ! -f "${initrd}" ]; then
              warn "No initrd-image ${initrd} found - skipping"
              continue
          fi

          warn "  Copying kernel and creating boot-entry for ${kver}"
          KERNEL_ESP_DIR="${PMX_ESP_DIR}/${kver}"
          KERNEL_LIVE_DIR="${esp}/${KERNEL_ESP_DIR}"
          mkdir -p "${KERNEL_LIVE_DIR}"
          cp -u --preserve=timestamps "${linux_image}" "${KERNEL_LIVE_DIR}/"
          cp -u --preserve=timestamps "${initrd}" "${KERNEL_LIVE_DIR}/"

          # create loader entry
          cat > "${esp}/loader/entries/proxmox-${kver}.conf" <<- EOF
                  title    ${LOADER_TITLE}
                  version  ${kver}
                  options  ${CMDLINE}
                  linux    /${KERNEL_ESP_DIR}/vmlinuz-${kver}
                  initrd   /${KERNEL_ESP_DIR}/initrd.img-${kver}
          EOF
      done
      # [...]
    #+end_src
    For us, the cat part is especially interesting: the =CMDLINE= variable
    in the line beginning with "=options=" contains the boot options for the
    Linux kernel. This variable is assigned in the same file:
    #+begin_src bash
      # [...]
      if [ -f /etc/kernel/cmdline ]; then
        CMDLINE="$(cat /etc/kernel/cmdline)"
      else
        warn "No /etc/kernel/cmdline found - falling back to /proc/cmdline"
        CMDLINE="$(cat /proc/cmdline)"
      fi
      # [...]
    #+end_src
    Apparently =/etc/kernel/cmdline= is the place where Proxmox stores it's
    boot options. The file contains one single line:
    #+begin_example 
      root=ZFS=rpool/ROOT/pve-1 boot=zfs
    #+end_example
    After finding the =/etc/kernel/cmdline= file, I did a bit of searching
    and according to the Proxmox [[https://pve.proxmox.com/pve-docs/pve-admin-guide.html#sysboot_edit_kernel_cmdline][documentation]], it is actually the
    apropriate file to change in this case.

    Now that we have identified the file we can use to configure our
    kernel options, there are two things we want to add:
    1. we want to make sure the network interface comes up so that we can
       ssh into the initramfs, we will use the =ip= option for that. It uses
       the following format (look [[https://www.kernel.org/doc/Documentation/filesystems/nfs/nfsroot.txt][here]] for further reading):
       #+begin_example
         ip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:
       <dns0-ip>:<dns1-ip>:<ntp0-ip>:
       #+end_example
       I omitted everything after autoconf, something like this works for
       me:
       #+begin_example
         ip=1.2.3.4::1.2.3.1:255.255.255.0:caliban:enpXsY:none:
       #+end_example
    2. also we have to tell the kernel which devices the cryptodevices are
       that we want to unlock, which is done using the =cryptodevice= option
       (here we have to supply the PARTUUIDs for both of our harddrives):
       #+begin_example
         cryptdevice=UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX cryptdevice=UUID=YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY
       #+end_example

    The whole content of =/etc/kernel/cmdline= looks like this:
    #+begin_example
      ip=1.2.3.4::1.2.3.1:255.255.255.0:caliban:enpXsY:none: cryptdevice=UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX cryptdevice=UUID=YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY root=ZFS=rpool/ROOT/pve-1 boot=zfs
    #+end_example

    The last thing to do is to:
    #+begin_src bash
      update-initramfs -u -k all
      pve-efiboot-tool refresh
    #+end_src

    Now you should be able to reboot your machine and ssh into the
    busybox on the port you just configured for =dropbear=. From there
    you can unlock the drives by running something like
    this{{{sidenote(You'll have to input it twice since you have two
    encrypted drives)}}}:
    #+begin_example bash
      echo -n "password" > /lib/cryptsetup/passfifo
    #+end_example
    Or:
    #+begin_example bash
      /lib/cryptsetup/askpass "password: " > /lib/cryptsetup/passfifo
    #+end_example

    Or you can also use the =cryptroot-unlock= script that is preinstalled
    already, which also prompts you to enter the password twice.

    If you're lazy, you can also use put the following script into
    =/etc/initramfs-tools/hooks= and make it executable. I basically
    merged the above example of using =/lib/cryptsetup/askpass= with a
    version of a unlock script I had lying around, it looks like it
    might have been from this [[https://gist.github.com/gusennan/712d6e81f5cf9489bd9f][gist]]. It asks you for a passphrase and
    then uses echo to write it into =/lib/cryptsetup/passfifo= twice
    (since I use 2 harddrives) with one second delay in between, then
    kills the session so the system can come up{{{sidenote( I
    noticed\, that /etc/motd\, which contains instructions on how to
    unlock your drive is not displayed in the busybox
    session.)}}}. You probably shouldn't use it, but it seems to work
    for me:
    #+begin_src bash
      #!/bin/sh

      PREREQ="dropbear"

      prereqs() {
          echo "$PREREQ"
      }

      case "$1" in
          prereqs)
              prereqs
              exit 0
              ;;
      esac

      . "${CONFDIR}/initramfs.conf"
      . /usr/share/initramfs-tools/hook-functions

      if [ "${DROPBEAR}" != "n" ] && [ -r "/etc/crypttab" ] ; then

          cat > "${DESTDIR}/bin/unlock" << EOF
      #!/bin/sh
      unlock_devices() {
        pw="\$(/lib/cryptsetup/askpass "password: ")"
        echo -n \$pw > /lib/cryptsetup/passfifo
        sleep 1
        echo -n \$pw > /lib/cryptsetup/passfifo
      }
      if unlock_devices; then
      # kill \`ps | grep cryptroot | grep -v "grep" | awk '{print \$1}'\`
      # following line kill the remote shell right after the passphrase has
      # been entered.
      kill -9 \`ps | grep "\-sh" | grep -v "grep" | awk '{print \$1}'\`
      exit 0
      fi
      exit 1
      EOF

          chmod 755 "${DESTDIR}/bin/unlock"

          mkdir -p "${DESTDIR}/lib/unlock"
          cat > "${DESTDIR}/lib/unlock/plymouth" << EOF
      #!/bin/sh
      [ "\$1" == "--ping" ] && exit 1
      /bin/plymouth "\$@"
      EOF

          chmod 755 "${DESTDIR}/lib/unlock/plymouth"

          echo To unlock root-partition run "unlock" >> ${DESTDIR}/etc/motd
      fi
    #+end_src
    That's pretty much all of it, you can now start enjoying remote
    reboots on your freshly encrypted Proxmox host.
** Proof of Concept: Adding Boot Environments to Proxmox VE 6
   :PROPERTIES:
   :EXPORT_FILE_NAME: poc-proxmox-and-boot-environments
   :EXPORT_HUGO_TAGS: proxmox zfs systemd-boot "boot environments"
   :EXPORT_HUGO_CODE_FENCE: t
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :date "\"2019-08-28\""
   :END:
   Dear Reader, this time I would like to invite you onto a small
   journey: To boldly go where no man has gone
   before{{{sidenote(Alright\, that's not true\, but I think it's the
   first time someone documents this kind of thing in the context of
   Proxmox)}}}. We're about to embark on a journey to make your Proxmox
   host quite literally immortal. Also since what we are essentially
   doing here is only a Proof of concept, you probably shouldn't use it
   in production, but as it's really amazing, so you might want to try
   it out in a test environment.

   In this article we are going to take a closer look at how Proxmox
   sets up the =ESP= {{{sidenote(EFI Systems Partition)}}} for
   =systemd-boot= and how we can adapt this process to support =boot
   environments=. Also this is going to be a long one, so you might want
   to grab a cup of coffee and some snacks to eat {{{marginnote(And
   maybe start installing a Proxmox VE 6 VM with ZFS\, because if boot
   environments are still new to you\, at the point when you've read
   about halfway through this post\, you will be eager to get your
   hands dirty and try this out for yourself)}}}.
*** Overview
    - What are Boot Environments?
      - Boot Environments on Linux
    - Poking around in Proxmox
      - The Proxmox ZFS Layout
      - The Boot Preparation
      - A Simple Proof of Concept
    - From one Proof of Concept to Another
      - Sidenote: The Proxmox ESP Size
      - =zedenv=: A Boot Environment Manager
      - =systemd-boot= and the EFI System Partitions
      - Making =zedenv= and Proxmox play well together
    - Conclusion and Future Work
*** What are Boot Environments?
    Boot environments are a truly amazing feature, which originated
    somewhere in the Solaris/Illumos ecosystem{{{sidenote(They have
    literally been around for ages\, I'm not quite sure at which point
    in time they were introduced\, but you can finde evidence [[https://books.google.com/books?id=8vrwjLsPkgwC&pg=PA109][at
    archeological digsites]] dating them back to at least 2003.)}}} and
    has since been adapted by other operating systems, such as FreeBSD,
    DragonflyBSD and others. The concept is actually quite simple:

    #+begin_quote
    A boot environment is a bootable Oracle Solaris environment
    consisting of a root dataset and, optionally, other datasets
    mounted underneath it. Exactly one boot environment can be active
    at a time.
    - [[https://docs.oracle.com/cd/E23824_01/html/E21801/index.html][Oracle Solaris 11 Information Library]]
    #+end_quote

    In my own words, I would describe boot environments as snapshots of
    a [partial] system, which can booted from (that is when a Boot
    Environment is active) or be mounted at runtime (by the same
    system).

    This enables a bunch of very interesting use-cases:
    - Rollbacks: This might not seem to be a pretty back deal at first,
      but once you realize that even after a major OS version upgrade,
      when something is suddenly broken, the previous version is just a
      reboot away.
    - You can create bootable system snapshots on your bare metal
      machines, not only on your virtual machines.
    - You can choose between creating a new boot environment to save
      the current systems state before updating or create a new boot
      environment, chroot into it, upgrade and reboot into a freshly
      upgraded system.
    - You can quite literally take your work home if you like, by
      creating a boot environment and **drum-roll** taking it home. You
      you can of course also use this in order to create a virtual
      machine, container, jail or zone of your system in order to test
      something new or for forensic purposes.

    Are you hooked yet? Good, you really should be. If you're not
    hooked, read till the end of the next section, you will
    be. {{{marginnote(If you're interested in boot environments\, I
    would suggest\, you take a look at vermadens [[https://vermaden.files.wordpress.com/2018/07/pbug-zfs-boot-environments-2018-07-30.pdf][presentation on ZFS
    boot environments]]\, or generally searching a bit on the web for
    articles about Boot Environments on other unix systems\,
    particularly there is quite a bit to be read on FreeBSD\, which
    recently adopted them and which is far more in depth and better
    explained than what I'll probably write down here.)}}}
**** Boot Environments on Linux
     While other operating systems have happily adapted boot
     environments, there is surprisingly{{{sidenote(Or maybe not so
     surprisingly\, if you remember how long zones and jails have been
     a thing\, while linux just recently started doing containers. At
     least there's still Windows to compare with.)}}} apparently not
     too much going on in the linux world. The focus here seems to be
     more on containerizing applications in order to isolate them from
     the rest of the host system rather than to make the host system
     itself more solid (which is also great, but not the same).

     On linux there are presently - at least to my knowledge - only the
     following projects that aim in a similiar direction:
     - There is [[https://en.opensuse.org/openSUSE:Snapper_Tutorial][snapper]] for =btrfs=, which seems to be a quite Suse
       specific solution. However according to it's documentation:
       [[https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_snapper_snapshot-boot.html#sec_snapper_snapshot-boot_limits]["A
       complete system rollback, restoring the complete system to the
       identical state as it was in when a snapshot was taken, is not
       possible."]] This, at least without more explanation or context
       sounds quite a bit spooky.
     - There is a [[https://github.com/b333z/beadm][Linux port]] of the FreeBSD beadm tool, which hasn't
       been updated in ~3 years, while beadm has. It does not seem to
       be maintained any more and to be tailored to a single gentoo
       installation.
     - There are a few =btrfs= specific scripts by a company called
       [[https://github.com/PluribusNetworks/pluribus_linux_userland/tree/master/components/bootenv-tools/bootenv-tools-src][Pluribus Networks]], which seem to have implemented their own
       version of =beadm= on top of =btrfs=. This apparently runs on some
       network devices.
     - [[https://nixos.org/][NixOS]] does something similiar to boot environments with their
       atomic update and rollback feature, but as far as I've
       understood this is still different from boot environments. Being
       functional, they don't exactly roll back to a old version of the
       system based on a filesystem snapshot, but rather recreate an
       identical environment to a previous one.
     - And finally there is [[https://github.com/johnramsden/zedenv][zedenv]], a boot environment manager that is
       written in python, supports both Linux and Freebsd and works
       really nice. It's also the one that I've used before. It is also
       what we are going to use here, since there really isn't an
       alternative when it comes to linux and ZFS.
*** Poking around in Proxmox
    But before we start grabbing a copy of =zedenv=, we have to take a
    closer look into Proxmox itself in order to look at what we may
    have to adapt.

    Basically we already know that it is generally possible to use boot
    environments with ZFS and linux, so what we want is hopefully not
    exactly rocket science.

    What we are going to check is:
    1. How do we have to adapt the Proxmox VE 6 rpool?
    2. How does Proxmox prepare the boot process and what do we have to
       tweak to make it boot into a boot environment?
**** The Proxmox ZFS layout
     In this part we are going to take a look at how the ZFS layout is
     set up by the proxmox installer. This is because there's a few
     things we have to consider when we use boot environments with
     Proxmox:
     1. We do not ever want to interfere in the operation of our guest
        machines: Since we have the ability to snapshot and restore
        virtual machines and containers, there is really no benefit to
        include them into the snapshots of our boot environments, on
        the contrary, we really don't want to end up with guests of our
        tenants missing files just because we've made a rollback.
     2. Is the ZFS layout compatible with running boot environments? Not
        all systems with ZFS are automatically compatible with using Boot
        Environments, basically if you just mount your ZFS pool as =/=, it
        won't work
     3. Are there any directories we have to exclude from the root
        dataset?

     So lets look at Proxmox:
     By default after installing with ZFS root you get a pool called
     =rpool= which is split up into =rpool/ROOT= as well as =rpool/data= and
     looks similiar to this (=zfs list=):

     #+begin_example bash
       rpool                         4.28G   445G      104K  /rpool
       rpool/ROOT                    2.43G   445G       96K  /rpool/ROOT
       rpool/ROOT/pve-1              2.43G   445G     2.43G  /
       rpool/data                    1.84G   445G      104K  /rpool/data
       rpool/data/subvol-101-disk-0   831M  7.19G      831M  /rpool/data/subvol-101-disk-0
       rpool/data/vm-100-disk-0      1.03G   445G     1.03G  -
     #+end_example

     =rpool/data= contains the virtual machines as well as the containers
     as you can see in the output of =zfs list= above. That's great, we
     don't have to manually move them. This takes care of the second
     point of our checklist from above.

     Also =rpool/ROOT/pve-1= is mounted as =/=, so we have =rpool/ROOT= which
     can potentially hold more than one snapshot of =/=, that is actually
     exactly what we need in order to use boot environments, the
     Proxmox team just saved us a bunch of time!

     This only leaves the third part of our little checklist open. Which
     directories are left that we don't want to snapshot as part of our
     boot environments? We can find a pretty important one in this
     context by checking =/etc/pve/storage.cfg=:

     #+begin_example
       dir: local
               path /var/lib/vz
               content iso,vztmpl,backup

       zfspool: local-zfs
               pool rpool/data
               sparse
               content images,rootdir
     #+end_example

     So while the virtual machines and the containers are part of
     =rpool/data=, iso files, templates and backups are still located in
     =rpool/root/pve-1=. That's not really what we want, imagine rolling
     back to a Boot Environment from a week ago and suddenly missing a
     weeks worth of Backups, that would be pretty annoying. Iso files
     as well as container templates are probably not worth keeping in
     our boot environments either.

     So lets take =/var/lib/vz= out of =rpool/root/pve-1=, first create a
     new dataset:
     #+begin_example bash
       root@caliban:/var/lib# zfs create -o mountpoint=/var/lib/vz rpool/vz
       cannot mount '/var/lib/vz': directory is not empty
     #+end_example

     Then move over the content of =/var/lib/vz= into the newly created
     and not yet mounted dataset:
     #+begin_example bash
       mv /var/lib/vz/ vz.old/ && zfs mount rpool/vz && mv vz.old/* /var/lib/vz/ && rmdir vz.old
     #+end_example

     If you don't have any images, templates or backups yet, or you just
     don't particularly care about them, you can of course also just
     remove =/var/lib/vz/*= entirely, mount =rpool/vz= and recreate the
     folder structure:
     #+begin_example bash
       root@caliban:~# tree -a /var/lib/vz/
       /var/lib/vz/
       ├── dump
       └── template
           ├── cache
           ├── iso
           └── qemu
     #+end_example

     Ok, now that's out of the way, we should in general be able to make
     snapshots, roll them back without disturbing the operation of the
     proxmox server too much.

     *BUT*: this might not apply to your server, since there is still a
     lot of other stuff in =/var/lib/= that you may want to include or
     exclude from snapshots! Better be sure to check what's in there.

     Also there are some other directories we might want to
     exclude. There is for example =/tmp= as well as =/var/tmp/= which
     shouldn't include anything that is worth keeping, but which of
     course would be snapshotted as well, we can create datasets for them
     as well and they should be automounted on reboot:
     #+begin_example bash
      zfs create -o mountpoint=/tmp     rpool/tmp
      zfs create -o mountpoint=/var/tmp rpool/var_tmp
     #+end_example

     If you've users that can connect directly to your Proxmox host,
     you might want to exclude =/home/= as well. =/root/= might be another
     good candidate, you may want to keep all of your shell history
     available at all times and regardless of which snapshot you're
     currently in. You can also think about whether or not you want to
     have your logs, mail and proabably a bunch of other things
     included or excluded, I guess both variants have their use cases.

     On my system =zfs list= returns something like this:
     #+begin_example bash
       NAME                           USED  AVAIL     REFER  MOUNTPOINT
       rpool                         5.25G   444G      104K  /rpool
       rpool/ROOT                    1.34G   444G       96K  /rpool/ROOT
       rpool/ROOT/pve-1              1.30G   444G     1.16G  /
       rpool/data                    2.59G   444G      104K  /rpool/data
       rpool/home_root               7.94M   444G     7.94M  /root
       rpool/tmp                      128K   444G      128K  /tmp
       rpool/var_tmp                  136K   444G      136K  /var/tmp
       rpool/vz                      1.30G   444G     1.30G  /var/lib/vz
     #+end_example

     At this point we've made sure that:
     1. the Proxmox ZFS layout is indeed compatible with Boot
        Environments pretty much out of the box
     2. we moved the directories that might impact day to day operations
        out of what we want to snapshot
     3. we also excluded a few more directories, which is optional
**** The Boot Preparation
     So after we've made sure that our ZFS layout works in this step we
     have to take a closer look at how the boot process is prepared in
     Proxmox. That is because as you might have noticed Proxmox does
     this a bit different from what you might be used to from other
     linux systems.

     As an example this is what =lsblk= looks like on my local machine:
     #+begin_example
       nvme0n1           259:0    0  477G  0 disk
       ├─nvme0n1p1       259:1    0    2G  0 part  /boot/efi
       └─nvme0n1p2       259:2    0  475G  0 part
         └─crypt         253:0    0  475G  0 crypt
           ├─system-swap 253:1    0   16G  0 lvm   [SWAP]
           └─system-root 253:2    0  100G  0 lvm   /
      #+end_example

     And this is =lsblk= on Proxmox:
     #+begin_example
       sda               8:0    0 465.8G  0 disk
       ├─sda1            8:1    0  1007K  0 part
       ├─sda2            8:2    0   512M  0 part
       └─sda3            8:3    0 465.3G  0 part
         └─cryptrpool1 253:0    0 465.3G  0 crypt
       sdd               8:48   0 465.8G  0 disk
       ├─sdd1            8:49   0  1007K  0 part
       ├─sdd2            8:50   0   512M  0 part
       └─sdd3            8:51   0 465.3G  0 part
         └─cryptrpool2 253:1    0 465.3G  0 crypt
     #+end_example

     Notice how there is no mounted EFI Systems Partition? That's
     because both{{{sidenote(Actually the UUIDs of all used ESP
     Partitions are stored in /etc/kernel/pve-efiboot-uuids)}}} of the
     /dev/sdX2 devices, which are involved holding my mirrored =proot=
     pool contain a valid ESP. Also proxmox does not mount these
     partitions by default but rather encurages the use of their
     =pve-efiboot-tool=, which then takes care of putting a valid boot
     configuration on all involved drives, so you can boot off any of
     them.

     This is not at all bad design, on the contrary, it is however
     noteworthy, because it bit it is different from what other systems
     with boot environments are using.

     Here is a quick recap on how in Proxmox the boot process is
     prepared:
     1. Initially something happens that requires an update of the
        bootloader configuration (e.g. a new kernel is installed or
        you've just set up an full disk encryption, changed something
        in the initramfs)
     2. This leads to =/usr/sbin/pve-efiboot-tool refresh= being run
        (either automated or manually), which at some point executes
        =/etc/kernel/postinst.d/zz-pve-efiboot=, which is the script that
        loops over the ESPs (which are defined by their UUID in
        =/etc/kernel/pve-efiboot-uuids=), mounts them and generates the
        boot loader configuration on them according to what Proxmox (or
        you as the user) has [[https://pve.proxmox.com/wiki/Host_Bootloader][defined as kernel versions to keep]]. The
        bootloader configuration is created for every kernel and
        configured with the kernel commandline options from
        =/etc/kernel/cmdline=.
     3. On reboot you can use any harddrive that holds a EFI System
        Partition to boot from.

     Incidentally the =/etc/kernel/cmdline= file is also the one we
     configured in the previous post in order to enable remote
     decryption on a fully encrypted Proxmox host. Apart from the the
     options we added to it last time, it also contains another very
     interesting one:

     #+begin_example bash
       root=ZFS=rpool/ROOT/pve-1
     #+end_example
**** A Simple Proof of Concept
     At this point we already have everything we need:
     #+begin_src bash
       zfs snapshot rpool/ROOT/pve-1@test
       zfs clone rpool/ROOT/pve-1@test rpool/ROOT/pve-2
       zfs set mountpoint=/ rpool/ROOT/pve-2
       sed -i 's/pve-1/pve-2/' /etc/kernel/cmdline
       pve-efiboot-tool refresh
       reboot
     #+end_src

     Tadaa!
     #+begin_example bash
       root@caliban:~# mount | grep rpool
       rpool/ROOT/pve-2 on / type zfs (rw,relatime,xattr,noacl)rpool on /rpool type zfs (rw,noatime,xattr,noacl)rpool/var_tmp on /var/tmp type zfs (rw,noatime,xattr,noacl)rpool/home_root on /root type zfs (rw,noatime,xattr,noacl)
       rpool/tmp on /tmp type zfs (rw,noatime,xattr,noacl)
       rpool/vz on /var/lib/vz type zfs (rw,noatime,xattr,noacl)
       rpool/ROOT on /rpool/ROOT type zfs (rw,noatime,xattr,noacl)
       rpool/data on /rpool/data type zfs (rw,noatime,xattr,noacl)
       rpool/data/subvol-101-disk-0 on /rpool/data/subvol-101-disk-0 type zfs (rw,noatime,xattr,posixacl)
     #+end_example

     Congratulations, you've just created your first boot environment!
     If you're not convinced yet, just install something such as =htop=,
     enjoy the colors for a bit and run:

     #+begin_example bash
       sed -i 's/pve-2/pve-1/' /etc/kernel/cmdline
       pve-efiboot-tool refresh
       reboot
     #+end_example

     And finally try to run =htop= again. Notice how it's not only gone,
     in fact it was never even there in the first place, at least in
     from the systems point of view! Let that sink in for a moment. You
     want this. {{{marginnote(At this point you might want to take a
     small break\, grab another cup of coffee\, lean back and remember
     this one time\, back in the day when you were just getting started
     with all this operations stuff\, it was almost beer o'clock and
     before going home you just wanted to apply this one tiny little
     update\, which of course led to the whole server
     breaking. Remember how\, when you were refilling your coffee cup
     for the third time this old solaris guru walked by on his way home
     and he had this mysterious smile on his face. Yeah\, he knew you
     were about to spend half of the night there fixing the issue and
     reinstalling everything\, in fact he probably had a similiar issue
     at the same day\, but then decided to just roll back go home a bit
     early and take care of it the next day.)}}}
*** From one Proof of Concept to Another
    So at this point we know how to set up a boot environment by hand,
    that's nice, but currently we only can hop back and forth between a
    single boot environment, which is not cool enough yet.

    We basically need some tooling which we can use to make everything
    work together nicely. 

    So in this section we are going to look into tooling as well as
    into how we may be able to make Proxmox play well together with the
    boot environment manager of our (only) choice =zedenv=.

    Our new objective is to look at what we need to do in order to
    enable us to select and start any number of boot environments from
    the boot manager.
**** Sidenote: The Proxmox ESP Size
     But first a tiny bit of math: since Proxmox uses systemd-boot, the
     kernel and initrd are stored in the EFI Systems Partition, which
     in a normal installation is 512MB in size. That should be enogh
     for the default case, where Proxmox stores only a hand full of
     kernels to boot from.

     In our case however we might want to be able to access a higher
     number of kernels, so we can travel back in time in order to also
     start old boot environments.

     A typical pair of kernel and initrd seems to be about 50MB in
     size, so we can currently store about 10 different kernels at a
     time.

     If we want to increase the size of the ESP however we might be out
     of luck, since ZFS does not like to shrink, so if you're in the
     situation of setting up a fresh Proxmox host, you might want to
     plug in a USB Stick (with less storage than your drives) or
     something similiar and create a mirrored ZFS RAID1 with this
     device and the other two drives which you really want to use for
     storage. This way the size of the resulting ZFS partition will be
     smaller than the drives you actually use and after the initial
     installation you can just:
     1. Remove the small drive from =rpool=
     2. Remove the first normal drive from =rpool=, delete the ZFS
        partition, increase the size of the ESP to whatever you want,
        recreate a ZFS partition and readd it to =rpool=
     3. wait until =rpool= has resilvered the drive
     4. repeat 2. and 3. with your second drive.
**** =zedenv=: A Boot Environment Manager
     Now lets install =zedenv= {{{sidenote(Be sure to read the
     [[https://zedenv.readthedocs.io][documentation]] at some point in time. Also check out [[https://ramsdenj.com][John Ramsdens
     blog]]\, which contains a bit more info about zedenv\, workint linux
     ZFS configuration and a bunch of other awesome stuff)}}}:

     #+begin_example bash
      root@caliban:~# apt install git python3-venv

      root@caliban:~# mkdir opt
      root@caliban:~# cd opt
      root@caliban:~# git clone https://github.com/johnramsden/pyzfscmds
      root@caliban:~# git clone https://github.com/johnramsden/zedenv

      root@caliban:~/opt# python3.7 -m venv zedenv-venv
      root@caliban:~/opt# . zedenv-venv/bin/activate
      (zedenv-venv) root@caliban:~/opt# cd pyzfscmds/
      (zedenv-venv) root@caliban:~/opt/pyzfscmds# python setup.py install
      (zedenv-venv) root@caliban:~/opt/pyzfscmds# cd ../zedenv
      (zedenv-venv) root@caliban:~/opt/zedenv# python setup.py install
    #+end_example

     Now =zedenv= should be installed into our new =zedenv-venv=:

     #+begin_example bash

       (zedenv-venv) root@caliban:~# zedenv --help
       Usage: zedenv [OPTIONS] COMMAND [ARGS]...
         ZFS boot environment manager cli
       Options:  --version
         --plugins  List available plugins.  --help     Show this message and exit.

       Commands:
         activate  Activate a boot environment.  create    Create a boot environment.
         destroy   Destroy a boot environment or snapshot.
         get       Print boot environment properties.  list      List all boot environments.
         mount     Mount a boot environment temporarily.
         rename    Rename a boot environment.  set       Set boot environment properties.
         umount    Unmount a boot environment.
     #+end_example

     As you can check =systemd-boot= seems to be supported out of the box:
     #+begin_example bash
       (zedenv-venv) root@caliban:~# zedenv --plugins
       Available plugins:
       systemdboot
     #+end_example

     But since we are using Proxmox =systemd-boot= and =zedenv= are
     actually not really supported. Remember that Proxmox doesn't
     actually mount the EFI System Partitions? Well =zedenv= makes the
     assumption that there is only one ESP, and that it is mounted
     somewhere at all times.

     Nonetheless, lets explore =zedenv= a bit so you can see how using a
     boot environment manager looks like. Let's =list= the available boot
     environments:
     #+begin_example bash
       (zedenv-venv) root@caliban:~# zedenv list
       Name   Active   Mountpoint   Creation
       pve-1  NR       /            Mon-Aug-19-1:27-2019

       (zedenv-venv) root@caliban:~# zfs list -r rpool/ROOT
       NAME               USED  AVAIL     REFER  MOUNTPOINT
       rpool/ROOT        1.17G   444G       96K  /rpool/ROOT
       rpool/ROOT/pve-1  1.17G   444G     1.17G  /
     #+end_example

     Before we can create new boot environments, we have to outwit
     =zedenv= on our Proxmox host: we have to set the bootloader to
     systemd-boot and due to the assumption that the ESP has to be
     mounted, we also have to make =zedenv= believe that the ESP is
     mounted (=/tmp/efi= is a reasonably sane path for this since we
     won't be really using =zedenv= to configure =systemd-boot= here):
     #+begin_example bash
       zedenv set org.zedenv:bootloader=systemdboot
       mkdir /tmp/efi
       zedenv set org.zedenv.systemdboot:esp=/tmp/efi
     #+end_example

     We can now =create= new boot environments:
     #+begin_example bash
       (zedenv-venv) root@caliban:~# zedenv create default-000

       (zedenv-venv) root@caliban:~# zedenv list
       Name         Active   Mountpoint   Creation
       pve-1        NR       /            Mon-Aug-19-1:27-2019
       default-000           -            Sun-Aug-25-19:44-2019

       (zedenv-venv) root@caliban:~# zfs list -r rpool/ROOT
       NAME                     USED  AVAIL     REFER  MOUNTPOINT
       rpool/ROOT              1.17G   444G       96K  /rpool/ROOT
       rpool/ROOT/default-000     8K   444G     1.17G  /
       rpool/ROOT/pve-1        1.17G   444G     1.17G  /
     #+end_example

     Notice the =NR=? This shows us that the =pve-1= boot environment is
     now active (=N=) and after the next reboot the =pve-1= boot
     environment will be active (=R=).

     We also get information on the mountpoint of the boot environment
     as well as the date, when the boot environment was created, so we
     get a bit more information than only having the name of the boot
     environment.

     On a fully supported system we could now also =activate= the
     =default-000= boot environment, that we've just created and we
     would then get an output similiar to this, showing us that
     =default-000= would be active on the next
     reboot{{{marginnote(=zedenv= can also =destroy=\, =mount= and =unmount=
     boot environments as well as =get= and =set= some ZFS specific
     options\, but right now what we want to focus on is how to get
     activation working with Proxmox.)}}}:
     #+begin_example bash
       (zedenv-venv) root@caliban:~# zedenv activate default-000
       (zedenv-venv) root@caliban:~# zedenv list
        Name         Active   Mountpoint   Creation
        pve-1        N        /            Mon-Aug-19-1:27-2019
        default-000  R        -            Sun-Aug-25-19:44-2019
     #+end_example

     Since we are on Proxmox however, instead we'll get the following
     error message:
     #+begin_example
       (zedenv-venv) root@caliban:~# zedenv activate default-000
       WARNING: Running activate without a bootloader. Re-run with a default bootloader, or with the '--bootloader/-b' flag. If you plan to manually edit your bootloader config this message can safely be ignored.
     #+end_example

     At this point you have seen how a typical boot environment manager
     looks like and you now know what =create= and =activate= will usually
     do. 
**** =systemd-boot= and the EFI System Partitions
     Next we'll take a closer look into the content of these EFI System
     Partitions and the files systemd-boot is using to start our system
     so lets take a look at what is stored on a ESP in Proxmox:
     #+begin_example bash

       (zedenv-venv) root@caliban:~# mount /dev/sda2 /boot/efi/
       (zedenv-venv) root@caliban:~# tree /boot/efi
       .
       ├── EFI
       │   ├── BOOT
       │   │   └── BOOTX64.EFI
       │   ├── proxmox
       │   │   ├── 5.0.15-1-pve
       │   │   │   ├── initrd.img-5.0.15-1-pve
       │   │   │   └── vmlinuz-5.0.15-1-pve
       │   │   └── 5.0.18-1-pve
       │   │       ├── initrd.img-5.0.18-1-pve
       │   │       └── vmlinuz-5.0.18-1-pve
       │   └── systemd
       │       └── systemd-bootx64.efi
       └── loader
           ├── entries
           │   ├── proxmox-5.0.15-1-pve.conf
           │   └── proxmox-5.0.18-1-pve.conf
           └── loader.conf
     #+end_example

     So we have the kernels and initrd in =EFI/proxmox= and some
     configuration files in =loader/=.

     The loader.conf file looks like this:
     #+begin_example bash
       (zedenv-venv) root@caliban:/boot/efi# cat loader/loader.conf
       timeout 3
       default proxmox-*
     #+end_example

     We have a 3 second timeout in =systemd-boot= and the default boot
     entry has to begin with the string =proxmox=. Nothing too
     complicated here.

     Apart from that, we have the =proxmox-5.X.X-pve.conf= files which we
     already know from last time (they are what is generated by the
     =/etc/kernel/postinst.d/zz-pve-efiboot= script). They look like
     this:
     #+begin_example
       (zedenv-venv) root@caliban:/boot/efi# cat loader/entries/proxmox-5.0.18-1-pve.conf
       title    Proxmox Virtual Environment
       version  5.0.18-1-pve
       options  ip=[...] cryptdevice=UUID=[...] cryptdevice=UUID=[...] root=ZFS=rpool/ROOT/pve-1 boot=zfs
       linux    /EFI/proxmox/5.0.18-1-pve/vmlinuz-5.0.18-1-pve
       initrd   /EFI/proxmox/5.0.18-1-pve/initrd.img-5.0.18-1-pve
     #+end_example
     So basically they just point to the kernel and initrd in the
     =EFI/proxmox= directory and start the kernel with the right =root=
     option so that the correct boot environment is mounted.

     At this point it makes sense to reiterate what a boot evironment
     is. Up until now we have defined a boot environment loosely as a
     file system snapshot we can boot into. At this point we have to
     refine the "we can boot into" part of this definition: A Boot
     environment is a filesystem snapshot together with the bootloader
     configuration as well as the kernel and initrd files from the
     moment the snapshot was taken.

     The boot environment of =pve-1= consists specifically of the
     following files from the ESP partition:
     #+begin_example
       .
       ├── EFI
       │   ├── proxmox
       │   │   ├── 5.0.15-1-pve
       │   │   │   ├── initrd.img-5.0.15-1-pve
       │   │   │   └── vmlinuz-5.0.15-1-pve
       │   │   └── 5.0.18-1-pve
       │   │       ├── initrd.img-5.0.18-1-pve
       │   │       └── vmlinuz-5.0.18-1-pve
       └── loader
           └── entries
               ├── proxmox-5.0.15-1-pve.conf
               └── proxmox-5.0.18-1-pve.conf
     #+end_example

     If you head over to the part of the [[https://zedenv.readthedocs.io/en/latest/plugins.html#systemdboot][zedenv documentation on
     systemd-boot]], you see that there the creation of an =/env= directory
     that holds all of the boot environment specific files on the ESP
     is proposed in that coupled with a bit of bind-mount magic tricks
     the underlying system into always finding the right files inside
     of =/boot=, when actually only the files that that belong to the
     currently active boot environment are mounted.

     This does not apply to our Proxmox situation, there is for example
     no mounted ESP. Also the =pve-efiboot-tool= takes care of the kernel
     versions that are available in the =EFI/proxmox/= directory so
     unless they are marked as manually installed (which you can do in
     Proxmox) some of the kernel versions will disappear at some point
     in time rendering the boot environment incomplete.

**** Making =zedenv= and Proxmox play well together

     I should probably point out here, that this part is more of a
     proposition, of how this could work than necessarily a good
     solution (it does work though). I'm pretty new to Proxmox and not
     at all an expert, when it comes to boot environments, so better
     take everything here with a few grains of salt.

     As we've learned in the previous part, =zedenv= is pretty awesome,
     but by design not exactly aimed at working with Proxmox. That
     being said, =zedenv= is actually written with plugins in mind, I've
     skimmed the code and there is a bunch of pre- and post-hooking
     going on, so I think it could be possible to just set up some sort
     of Proxmox plugin for =zedenv=. Since I'm not a =python= guy and
     there's of course also the option to add support to this from the
     Proxmox side, I'll just write down how I'd imagine Proxmox and a
     boot environment manager such as =zedenv= to work together without
     breaking too much on either side.

     For this we have to consider the following things:
     1. remember the =NR=? I guess =zedenv= just checks what is currently
        mounted as =/= in order to find out what the currently active
        boot environment (=N=) is. In Proxmox in order to find out what
        the active boot-environment after a reboot (=R=) will be, we can
        just check the =/etc/kernel/cmdline= file for the =root= option
     2. =activate= does not work with Proxmox, instead of creating the
        systemd-boot files, we could just run =pve-efiboot-tool refresh=,
        which creates a copy of the necessary bootloader files on all
        ESPs and doing so also activates the boot environment, that is
        referenced in the =/etc/kernel/cmdline= file. So with a template
        cmdline file like for example =/etc/kernel/cmdline.template=, we
        could run something like this, basically creating a cmdline,
        that points to the correct boot environment and refresh the
        content of all ESPs at the same time:
        #+begin_example bash
          sed 's/BOOTENVIRONMENT/pve-2/' /etc/kernel/cmdline.template > /etc/kernel/cmdline
          pve-efiboot-tool refresh
        #+end_example

     That's about everything we'd need to replace in order to get a
     single boot environment to work with =zedenv=.

     Now if we want to have access to multiple boot environments at the
     same time, we can just do something quite similiar to what Proxmox
     does: The idea here would be that after any run of
     =pve-efiboot-tool refresh=, we mount one of the ESPs and grab the
     all the files we need from =EFI/proxmox/= as well as =loader/entries/=
     and store them somewhere on =rpool=. We could for example create
     =rpool/be= for this exact reason.

     #+begin_example bash
       zfs create -o mountpoint=/be rpool/be
     #+end_example

     The initrd that comes with a kernel might be customized, so
     although the initrd file is built against a kernel we want to
     always keep it directly tied to a specific boot environments. I'm
     not sure if the kernel files change over time or not though, so
     there's two options here (Let's just assume the first of the
     following to be sure):
     - If they are changing over time, while keeping the same kernel
       version, it is probably best to save them per boot environment
     - If they don't change over time, we could just grab them once and
       then link to the kernel once we've already saved it

***** A better Proof of Concept

      So lets suppose that we have just freshly created the boot
      environment =EXAMPLE= and activated it as described above using
      =pve-efiboot-tool refresh= on a =/etc/kernel/cmdline= file that we
      derived from the =/etc/kernel/cmdline.template= and Proxmox has
      just set up all of our ESPs.

      Now we create a =/be/EXAMPLE/= directory, then mount one of the
      ESPs and in the next step copy over the files we are interested
      in.

      #+begin_example bash
        (zedenv-venv) root@caliban:~# BOOTENVIRONMENT=EXAMPLE
        (zedenv-venv) root@caliban:~# mkdir -p /be/$BOOTENVIRONMENT/{kernel,entries}
        (zedenv-venv) root@caliban:~# zedenv create $BOOTENVIRONMENT
        (zedenv-venv) root@caliban:~# zedenv list
        Name         Active   Mountpoint   Creation
        pve-1        NR       -            Mon-Aug-19-1:27-2019
        EXAMPLE               -            Wed-Aug-28-20:24-2019


        (zedenv-venv) root@caliban:~# pve-efiboot-tool refresh
        Running hook script 'pve-auto-removal'..
        Running hook script 'zz-pve-efiboot'..
        Re-executing '/etc/kernel/postinst.d/zz-pve-efiboot' in new private mount namespace..
        Copying and configuring kernels on /dev/disk/by-uuid/EE5A-CB7D
                Copying kernel and creating boot-entry for 5.0.15-1-pve
                Copying kernel and creating boot-entry for 5.0.18-1-pve
        Copying and configuring kernels on /dev/disk/by-uuid/EE5B-4F9B
                Copying kernel and creating boot-entry for 5.0.15-1-pve
                Copying kernel and creating boot-entry for 5.0.18-1-pve


        (zedenv-venv) root@caliban:~# mount /dev/disk/by-uuid/$(cat /etc/kernel/pve-efiboot-uuids | head -n 1) /boot/efi


        (zedenv-venv) root@caliban:~# cp -r /boot/efi/EFI/proxmox/* /be/$BOOTENVIRONMENT/kernel/
        (zedenv-venv) root@caliban:~# cp /boot/efi/loader/entries/proxmox-*.conf /be/$BOOTENVIRONMENT/entries/
      #+end_example

      Now that we have copied over all files we need, it's time to
      modify them =/be= should look like this:
      #+begin_example
        /be/
        └── EXAMPLE
            ├── entries
            │   ├── proxmox-5.0.15-1-pve.conf
            │   └── proxmox-5.0.18-1-pve.conf
            └── kernel
                ├── 5.0.15-1-pve
                │   ├── initrd.img-5.0.15-1-pve
                │   └── vmlinuz-5.0.15-1-pve
                └── 5.0.18-1-pve
                    ├── initrd.img-5.0.18-1-pve
                    └── vmlinuz-5.0.18-1-pve
      #+end_example

      Lets first fix the file names, =/entries/proxmox-X.X.X-X-pve.conf=
      should be named after our boot environment =EXAMPLE=:

      #+begin_example bash
        BOOTENVIRONMENT=EXAMPLE
        cd /be/EXAMPLE/entries/
        for f in *.conf; do mv $f $(echo $f | sed "s/proxmox/$BOOTENVIRONMENT/"); done
      #+end_example

      Next let's look into those configuration files:
      #+begin_example
        title    Proxmox Virtual Environment
        version  5.0.18-1-pve
        options  [...] root=ZFS=rpool/ROOT/EXAMPLE boot=zfs
        linux    /EFI/proxmox/5.0.18-1-pve/vmlinuz-5.0.18-1-pve
        initrd   /EFI/proxmox/5.0.18-1-pve/initrd.img-5.0.18-1-pve
      #+end_example

      As you can see they reference the initrd as well as the kernel
      (with the ESP being =/=). We are going to put these into
      =/env/EXAMPLE/= instead, so lets fix this. Also we want to change
      the Title from "Proxmox Virtual Environment" to the name of the
      boot environment.
      #+begin_example bash
        cd /be/EXAMPLE/entries

        BOOTENVIRONMENT=EXAMPLE
        sed -i "s/EFI/env/;s/proxmox/$BOOTENVIRONMENT/" *.conf
        sed -i "s/Proxmox Virtual Environment/$BOOTENVIRONMENT/" *.conf
      #+end_example

      The configuration files now look like this:
      #+begin_example bash
        (zedenv-venv) root@caliban:/be/EXAMPLE/entries# cat EXAMPLE-5.0.18-1-pve.conf
        title    EXAMPLE
        version  5.0.18-1-pve
        options  [...] root=ZFS=rpool/ROOT/EXAMPLE boot=zfs
        linux    /env/EXAMPLE/5.0.18-1-pve/vmlinuz-5.0.18-1-pve
        initrd   /env/EXAMPLE/5.0.18-1-pve/initrd.img-5.0.18-1-pve
      #+end_example

      Remember the =loader.conf= file that only consisted of 2 lines?
      We'll also need to have one like it in order to boot into our new
      boot environment by default, so we'll copy and modify it as well.
      #+begin_example bash
        BOOTENVIRONMENT=EXAMPLE
        cat /boot/efi/loader/loader.conf | sed "s/proxmox/$BOOTENVIRONMENT/" > /be/$BOOTENVIRONMENT/loader.conf
      #+end_example

      Finally don't forget to unmount the ESP:
      #+begin_example bash
        (zedenv-venv) root@caliban:~# umount /boot/efi/
      #+end_example

      Now =/be= should look similiar to this and at this point we are
      done with the part where we configure our files:
      #+begin_example bash
        (zedenv-venv) root@caliban:~# tree /be
        /be
        └── EXAMPLE
            ├── entries
            │   ├── EXAMPLE-5.0.15-1-pve.conf
            │   └── EXAMPLE-5.0.18-1-pve.conf
            ├── kernel
            │   ├── 5.0.15-1-pve
            │   │   ├── initrd.img-5.0.15-1-pve
            │   │   └── vmlinuz-5.0.15-1-pve
            │   └── 5.0.18-1-pve
            │       ├── initrd.img-5.0.18-1-pve
            │       └── vmlinuz-5.0.18-1-pve
            └── loader.conf
      #+end_example

      Next we can deploy our configuration to the ESP, to do so, we'll
      do the following:
      1. iterate over the ESPs used by Proxmox (remember their UUIDs are
         in =/etc/kernel/pve-efiboot-uuids=)
      2. mount a ESP
      3. copy the files in =/be/EXAMPLE/kernel= into =/env/EXAMPLE=
         {{{sidenote(remember that this =/= is the root of the ESP)}}
      4. copy the config files in =/be/EXAMPLE/entries= into
         =/loader/entries/=
      5. replace =/loader/loader.conf= with our modified =loader.conf= in
         order to activate the boot environment
      6. unmount the ESP
      7. repeat until we've updated all ESPs from
         =/etc/kernel/pve-efiboot-uuids=

      In =bash= this could look like this:
      #+begin_example bash
        BOOTENVIRONMENT=EXAMPLE

        for esp in $(cat /etc/kernel/pve-efiboot-uuids);
        do
          mount /dev/disk/by-uuid/$esp /boot/efi
          mkdir -p /boot/efi/env/$BOOTENVIRONMENT
          cp -r /be/$BOOTENVIRONMENT/kernel/* /boot/efi/env/$BOOTENVIRONMENT/
          cp /be/$BOOTENVIRONMENT/entries/* /boot/efi/loader/entries/
          cat /be/$BOOTENVIRONMENT/loader.conf > /boot/efi/loader/loader.conf
          umount /boot/efi
        done
      #+end_example

      Also note that reactivating the boot environment is now simply a
      matter of replacing the =loader.conf=.


      Now let's check if everything looks good. The ESPs should look
      like this:
      #+begin_example bash
        (zedenv-venv) root@caliban:~# tree /boot/efi/
        /boot/efi/
        ├── EFI
        │   ├── BOOT
        │   │   └── BOOTX64.EFI
        │   ├── proxmox
        │   │   ├── 5.0.15-1-pve
        │   │   │   ├── initrd.img-5.0.15-1-pve
        │   │   │   └── vmlinuz-5.0.15-1-pve
        │   │   └── 5.0.18-1-pve
        │   │       ├── initrd.img-5.0.18-1-pve
        │   │       └── vmlinuz-5.0.18-1-pve
        │   └── systemd
        │       └── systemd-bootx64.efi
        ├── env
        │   └── EXAMPLE
        │       ├── 5.0.15-1-pve
        │       │   ├── initrd.img-5.0.15-1-pve
        │       │   └── vmlinuz-5.0.15-1-pve
        │       └── 5.0.18-1-pve
        │           ├── initrd.img-5.0.18-1-pve
        │           └── vmlinuz-5.0.18-1-pve
        └── loader
            ├── entries
            │   ├── EXAMPLE-5.0.15-1-pve.conf
            │   ├── EXAMPLE-5.0.18-1-pve.conf
            │   ├── proxmox-5.0.15-1-pve.conf
            │   └── proxmox-5.0.18-1-pve.conf
            └── loader.conf
      #+end_example

      The =loader.conf= should default to =EXAMPLE= entries:
      #+begin_example bash
        (zedenv-venv) root@caliban:~# cat /boot/efi/loader/loader.conf | grep EXAMPLE
        default EXAMPLE-*
      #+end_example

      The =EXAMPLE-*.conf= files should point to the files in
      =/env/EXAMPLE=:
      #+begin_example bash
        (zedenv-venv) root@caliban:~# cat /boot/efi/loader/entries/EXAMPLE-*.conf | grep "env/EXAMPLE"
        linux    /env/EXAMPLE/5.0.15-1-pve/vmlinuz-5.0.15-1-pve
        initrd   /env/EXAMPLE/5.0.15-1-pve/initrd.img-5.0.15-1-pve
        linux    /env/EXAMPLE/5.0.18-1-pve/vmlinuz-5.0.18-1-pve
        initrd   /env/EXAMPLE/5.0.18-1-pve/initrd.img-5.0.18-1-pve
      #+end_example

      At this point all that's left is to reboot and check if
      everything works. And it does indeed:

      [[./img/bootenvironments.png]]

*** Conclusion and Future Work
    We've done it! It works and lets face it, it is an amazing feature,
    we all want to have this on our Hypervisors.

    But let's not forget that what we've done here is pretty much to
    write down two Proofs of Concepts. In order to make sure
    everything works nicely we probably need to at least add a bunch
    of checks.

    From my point of view there's basically two ways all of this could
    go forward:
    1. The =zedenv= project adopts support for the Proxmox platform by
       relaxing their assumption that the EFI Systems Partition has to
       be mounted and writes a bunch of gluecode around the Proxmox
       tooling.
    2. The Proxmox team adds native support for Boot Environments to
       their pve-tooling. This would mean that they would have to add
       all the functionality of a boot environment manager such as
       =zedenv= or it's unix counterpart =beadm=, they would also have to
       consider making EFI System Partition bigger, and the kernels and
       boot environments wouldn't necessarily require something like
       =rpool/be= to be stored on {{{sidenote(On the topic of not having
       to use some intermediate storage such as =rpool/be=\, having the
       boot environments stored additionally on zfs would enable to
       keep the ESP relatively small and not only distinguish between
       active and inactive boot environments\, but also between ones
       that are loaded onto the ESP and those that are merely
       available\, this isn't exactly what =zedenv= or =beadm= do\, but it
       might be a nice feature to really be able to go back to older
       version in the Enterprise context of Proxmox)}}}, but that
       should be trivial. Also I do like the idea of templating the
       =/etc/kernel/cmdline= file so that generating boot configurations
       works using the included pve tooling.

    Personally I think it would be great to get support for boot
    environments directly from Proxmox, since - let's face it - it's
    almost working out of the box anyway and a custom tool such as
    =pve-beadm= would better fit the way Proxmox handles the boot process
    than something that is build around it.

    Anyway that's about all from me this time. I'm now a few days into
    my little venture of 'just installing Proxmox because it will work
    out of the box which will save me some time' and I feel quite
    confident that I've almost reached the stage where I'm actually
    done with installing the host system and can start running some
    guests. There's literally only one or two things left to try out..

    If anyone at Proxmox reads this, you guys are amazing! I haven't
    even really finished my first install yet and I'm hooked with your
    system!
